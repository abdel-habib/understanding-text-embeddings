{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b6220f0-bf0f-49c5-92c2-9455cef116aa",
   "metadata": {},
   "source": [
    "As an initial stage, let's take a deeper look at thr `emilyalsentzer/Bio_ClinicalBERT` medical LLM model as a feature extractor from HF. The model has been pre-trained on clinical text from MIMIC-III v1.4 database.\n",
    "\n",
    "- model card: https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT\n",
    "- dataset paper trained with: https://www.nature.com/articles/sdata201635\n",
    "- model paper: https://arxiv.org/abs/1904.03323 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1a6aeb7-4d37-4c46-8995-4c304ba5299e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# for auto reload when changes are made in the package\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a449e535-208d-4240-b74e-9a0129d9d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from helpers import show_tokenization, search_through_vocab_dict_using_id, search_through_vocab_dict_using_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f894594-2466-47e0-a61d-d9d6ca28cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = 'emilyalsentzer/Bio_ClinicalBERT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e562340-7bc3-4b63-b25b-a9e446333677",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
    "model = AutoModel.from_pretrained(model_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b54e6ca-9918-46cd-8fce-e341ca5b2299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='emilyalsentzer/Bio_ClinicalBERT', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f56aec27-e5e1-4db4-bf51-0607d9aa2b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5844ac85-4db7-48b8-a3ac-ec47750dfc44",
   "metadata": {},
   "source": [
    "#### How many tokens can the tokenizer handle? How many embeddings can represent each token 'embedding matrix'?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b12318c-30b3-4d86-ab5c-0ffaff1dc85d",
   "metadata": {},
   "source": [
    "<img style=\"display: block;margin-left: auto;margin-right: auto;\" src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/0*luNBhHsLBbjMSHew.png\" alt=\"image info\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6af8345-f6ee-40c7-8fff-5e3bccbb7f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vocab size is: 28996\n",
      "the embedding dimension is: 768\n",
      "thus the size of the embedding matrix is therefore 28996 × 768\n"
     ]
    }
   ],
   "source": [
    "# number of unique tokens (words, subwords, or characters) that the tokenizer is capable of recognizing and mapping to an index in its vocabulary\n",
    "tokenizer_vocab_size = tokenizer.vocab_size\n",
    "print(f\"the vocab size is: {tokenizer_vocab_size}\") \n",
    "\n",
    "# extracting the embedding dimension for each token\n",
    "embedding_dim = model.config.hidden_size\n",
    "print(f\"the embedding dimension is: {embedding_dim}\")\n",
    "\n",
    "# (each token in the vocabulary is represented by a vector with 768 elements)\n",
    "print (f\"thus the size of the embedding matrix is therefore {tokenizer_vocab_size} × {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e4fa47-e68a-419f-bc34-6f3eea2b2909",
   "metadata": {},
   "source": [
    "##### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e796028b-a3cc-40fc-bef5-a6beda2dd853",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Patient has no history of fatigue, weight change, loss of appetite, or weakness.'\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5f8a2f74-4e9a-4063-bb99-a6d1a1c57dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  5351,  1144,  1185,  1607,  1104, 18418,   117,  2841,  1849,\n",
       "           117,  2445,  1104, 21518,   117,  1137, 11477,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7b4d11d-6427-444c-ae19-f4cee0044792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor(101)</td>\n",
       "      <td>[CLS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(5351)</td>\n",
       "      <td>patient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor(1144)</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor(1185)</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tensor(1607)</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tensor(1104)</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tensor(18418)</td>\n",
       "      <td>fatigue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tensor(117)</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tensor(2841)</td>\n",
       "      <td>weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tensor(1849)</td>\n",
       "      <td>change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tensor(117)</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tensor(2445)</td>\n",
       "      <td>loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tensor(1104)</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tensor(21518)</td>\n",
       "      <td>appetite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tensor(117)</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tensor(1137)</td>\n",
       "      <td>or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tensor(11477)</td>\n",
       "      <td>weakness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tensor(119)</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tensor(102)</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id     token\n",
       "0     tensor(101)     [CLS]\n",
       "1    tensor(5351)   patient\n",
       "2    tensor(1144)       has\n",
       "3    tensor(1185)        no\n",
       "4    tensor(1607)   history\n",
       "5    tensor(1104)        of\n",
       "6   tensor(18418)   fatigue\n",
       "7     tensor(117)         ,\n",
       "8    tensor(2841)    weight\n",
       "9    tensor(1849)    change\n",
       "10    tensor(117)         ,\n",
       "11   tensor(2445)      loss\n",
       "12   tensor(1104)        of\n",
       "13  tensor(21518)  appetite\n",
       "14    tensor(117)         ,\n",
       "15   tensor(1137)        or\n",
       "16  tensor(11477)  weakness\n",
       "17    tensor(119)         .\n",
       "18    tensor(102)     [SEP]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show each token and its respective id\n",
    "show_tokenization(inputs, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d5636a-c515-450a-b237-7841af8b6b92",
   "metadata": {},
   "source": [
    "What does the above table means? It means that each of the tokens are represented by the paired id value inside the tokenizer vocab_size items. <br>\n",
    "In other words, the word `history` is represented by a value of `1607` from the tokenizer (as a number as machines understand numbers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b208586c-3909-43d2-a4d8-112f48fce853",
   "metadata": {},
   "source": [
    "There are two ways to validate this to understand it more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9832693-2b53-43cf-a9ef-9092705e618a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first few tokens: [('Jersey', 3308), ('Bridges', 17501), ('Beth', 6452), ('Schumacher', 24656), ('calf', 23256), ('app', 12647), ('develops', 11926), ('##quisition', 18540), ('##ination', 9400), ('##zon', 9515)]\n"
     ]
    }
   ],
   "source": [
    "# here are some examples of the vocabularies and their paired ids from the tokenizer\n",
    "vocab_dict = tokenizer.get_vocab()\n",
    "print(\"first few tokens:\", list(vocab_dict.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6c022a79-42ad-43c5-8fd0-06ba715dead8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history\n",
      "history\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1607"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we want to decode 1607 for the word history, we can do the following\n",
    "print(tokenizer.decode(1607))\n",
    "\n",
    "# or search through all of the vocabularies based on the specific id\n",
    "print(search_through_vocab_dict_using_id(tokenizer, 1607))\n",
    "\n",
    "# or search using the token text value \n",
    "search_through_vocab_dict_using_token(tokenizer, 'history')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923fc21c-dc42-4d6a-83aa-219e7955f012",
   "metadata": {},
   "source": [
    "##### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e6f220ba-cb86-495a-91a4-9e53176b31c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,   170, 12477,  2646, 15454,  3367,  1114, 13102,  3571,  1110,\n",
      "          5085,  1113,  1103,  1268,  2458,  1104,  1103, 12477,  6262, 28012,\n",
      "           119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor(101)</td>\n",
       "      <td>[CLS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(170)</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor(12477)</td>\n",
       "      <td>ma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor(2646)</td>\n",
       "      <td>##li</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tensor(15454)</td>\n",
       "      <td>##gnant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tensor(3367)</td>\n",
       "      <td>mass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tensor(1114)</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tensor(13102)</td>\n",
       "      <td>oval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tensor(3571)</td>\n",
       "      <td>shape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tensor(1110)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tensor(5085)</td>\n",
       "      <td>visible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tensor(1113)</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tensor(1103)</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tensor(1268)</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tensor(2458)</td>\n",
       "      <td>view</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tensor(1104)</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tensor(1103)</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tensor(12477)</td>\n",
       "      <td>ma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tensor(6262)</td>\n",
       "      <td>##mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tensor(28012)</td>\n",
       "      <td>##ogram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tensor(119)</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tensor(102)</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id    token\n",
       "0     tensor(101)    [CLS]\n",
       "1     tensor(170)        a\n",
       "2   tensor(12477)       ma\n",
       "3    tensor(2646)     ##li\n",
       "4   tensor(15454)  ##gnant\n",
       "5    tensor(3367)     mass\n",
       "6    tensor(1114)     with\n",
       "7   tensor(13102)     oval\n",
       "8    tensor(3571)    shape\n",
       "9    tensor(1110)       is\n",
       "10   tensor(5085)  visible\n",
       "11   tensor(1113)       on\n",
       "12   tensor(1103)      the\n",
       "13   tensor(1268)    right\n",
       "14   tensor(2458)     view\n",
       "15   tensor(1104)       of\n",
       "16   tensor(1103)      the\n",
       "17  tensor(12477)       ma\n",
       "18   tensor(6262)     ##mm\n",
       "19  tensor(28012)  ##ogram\n",
       "20    tensor(119)        .\n",
       "21    tensor(102)    [SEP]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'A malignant mass with oval shape is visible on the right view of the mammogram.'\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "show_tokenization(inputs, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eed560e-125c-41f2-92b4-052523155546",
   "metadata": {},
   "source": [
    "Here we can see that there are some unknown words for the tokenizer. This prorcess is called `Subword tokenization` where an unknown word is divided into subwords. It can't represent the words `malignant` or `mammogram` as it is not in the vocabulary dictionary, thus it combines subwords. <br><br> We can validate it below by looking at each token text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "453de02a-472f-43e6-9bd9-21803b75197c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'ma',\n",
       " '##li',\n",
       " '##gnant',\n",
       " 'mass',\n",
       " 'with',\n",
       " 'oval',\n",
       " 'shape',\n",
       " 'is',\n",
       " 'visible',\n",
       " 'on',\n",
       " 'the',\n",
       " 'right',\n",
       " 'view',\n",
       " 'of',\n",
       " 'the',\n",
       " 'ma',\n",
       " '##mm',\n",
       " '##ogram',\n",
       " '.']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b876b3-2ea1-4a0c-97a1-17326931ea05",
   "metadata": {},
   "source": [
    "This is definitly a challenge for medical data, which could be out of the training dataset domain. The tokenizer's vocabulary might not contain the entire word \"malignant\", but it has subwords like 'ma', '##li', and '##gnant'. This way, it can still represent and process the word. <br><br> Here we validate as well: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "05a890fb-0056-4b8e-9d4e-c9baf7d7d7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Token not found in the vocabulary.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_through_vocab_dict_using_token(tokenizer, 'malignant')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
