{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee344986-6526-4ee1-a9a8-b6047a7f15d5",
   "metadata": {},
   "source": [
    "### After running the tokens through the model and obtaining them from the last hidden state, it is cruitial to extract the essential information from the resulted output. This notebook will walk through some important embedding pooling steps and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d2db8999-6a30-4095-94bb-64472f2fc15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# for auto reload when changes are made in the package\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7d39c990-531e-4cf1-82a0-401a5585e848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from helpers import show_tokenization, search_through_vocab_dict_using_id, search_through_vocab_dict_using_token, embedding_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "28b09e02-8a5b-4998-ba7f-911359d31119",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = 'emilyalsentzer/Bio_ClinicalBERT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "35e868cc-67bf-487d-b40c-2dc5cc13d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
    "model = AutoModel.from_pretrained(model_card)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d66839-563d-4782-be9b-a0ee9375a98c",
   "metadata": {},
   "source": [
    "Performing all steps: input > tokenization > model > output from `last_hidden_state`. One addition we add to experiment better is the `padding` and `max_length` to the tokenizer, where the number of tokens will be equal to the the defined max length, and the padded tokens will be given 0 with attention value equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "807d6dd7-ad0e-4989-9f08-1cc98a98e279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  5351,  1144,  1185,  1607,  1104, 18418,   117,  2841,  1849,\n",
      "           117,  2445,  1104, 21518,   117,  1137, 11477,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "total number of tokens is 19\n",
      "last_hidden_state outputs shape: torch.Size([1, 19, 768])\n"
     ]
    }
   ],
   "source": [
    "text = 'Patient has no history of fatigue, weight change, loss of appetite, or weakness.'\n",
    "inputs = tokenizer(text, return_tensors=\"pt\") #, padding=\"max_length\", max_length=512) #, padding=\"max_length\", max_length=512)\n",
    "\n",
    "print(inputs)\n",
    "print(f\"\\ntotal number of tokens is {len(inputs['input_ids'][0])}\")\n",
    "# show_tokenization(inputs, tokenizer)\n",
    "\n",
    "output = model(**inputs)['last_hidden_state'] # batch_size, sequence_length, hidden_size\n",
    "print(f'last_hidden_state outputs shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb988ec8-3f40-4ae3-9c74-744a9a7dee9c",
   "metadata": {},
   "source": [
    "To derive a single embedding from an LLM, you typically pool the hidden states using strategies like averaging the embeddings of all tokens, using the [CLS] tokenâ€™s embedding, or other methods such as max pooling. The pooling approach often depends on the task and model design. Attention masks are used during pooling to avoid the influence of padding tokens, but may be less relevant for strategies like [CLS]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f2fc541f-b0bf-417a-acc1-d12fbe4158ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_pooling = embedding_pooling(output, 'cls')\n",
    "eos_pooling = embedding_pooling(output, inputs['attention_mask'], 'eos')\n",
    "max_pooling = embedding_pooling(output, inputs['attention_mask'], 'max')\n",
    "mean_pooling = embedding_pooling(output, inputs['attention_mask'], 'mean')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
