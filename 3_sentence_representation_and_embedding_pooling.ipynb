{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee344986-6526-4ee1-a9a8-b6047a7f15d5",
   "metadata": {},
   "source": [
    "### After running the tokens through the model and obtaining them from the last hidden state, it is cruitial to extract the essential information from the resulted output. This notebook will walk through some important embedding pooling steps and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d2db8999-6a30-4095-94bb-64472f2fc15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# for auto reload when changes are made in the package\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7d39c990-531e-4cf1-82a0-401a5585e848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from helpers import show_tokenization, cosine_similarity, euclidean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "28b09e02-8a5b-4998-ba7f-911359d31119",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = 'emilyalsentzer/Bio_ClinicalBERT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "35e868cc-67bf-487d-b40c-2dc5cc13d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
    "model = AutoModel.from_pretrained(model_card)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d66839-563d-4782-be9b-a0ee9375a98c",
   "metadata": {},
   "source": [
    "Performing all steps: input > tokenization > model > output from `last_hidden_state`. One addition we add to experiment better is the `padding` and `max_length` to the tokenizer, where the number of tokens will be equal to the the defined max length, and the padded tokens will be given 0 with attention value equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "807d6dd7-ad0e-4989-9f08-1cc98a98e279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  5351,  1144,  1185,  1607,  1104, 18418,   117,  2841,  1849,\n",
      "           117,  2445,  1104, 21518,   117,  1137, 11477,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "total number of tokens is 19\n",
      "last_hidden_state outputs shape: torch.Size([1, 19, 768])\n"
     ]
    }
   ],
   "source": [
    "text = 'Patient has no history of fatigue, weight change, loss of appetite, or weakness.'\n",
    "inputs = tokenizer(text, return_tensors=\"pt\") #, padding=\"max_length\", max_length=512) #, padding=\"max_length\", max_length=512)\n",
    "\n",
    "print(inputs)\n",
    "print(f\"\\ntotal number of tokens is {len(inputs['input_ids'][0])}\")\n",
    "# show_tokenization(inputs, tokenizer)\n",
    "\n",
    "output = model(**inputs)['last_hidden_state'] # batch_size, sequence_length, hidden_size\n",
    "print(f'last_hidden_state outputs shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb988ec8-3f40-4ae3-9c74-744a9a7dee9c",
   "metadata": {},
   "source": [
    "To derive a single embedding from an LLM, you typically pool the hidden states using strategies like averaging the embeddings of all tokens, using the [CLS] tokenâ€™s embedding, or other methods such as max pooling. The pooling approach often depends on the task and model design. Attention masks are used during pooling to avoid the influence of padding tokens, but may be less relevant for strategies like [CLS]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee95beb-7a6f-468d-9ffb-7a73fe9657d3",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"./public/embedding-summary.png\" style=\"borders:none\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f2fc541f-b0bf-417a-acc1-d12fbe4158ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_pooling = embedding_pooling(output, 'cls')\n",
    "eos_pooling = embedding_pooling(output, inputs['attention_mask'], 'eos')\n",
    "max_pooling = embedding_pooling(output, inputs['attention_mask'], 'max')\n",
    "mean_pooling = embedding_pooling(output, inputs['attention_mask'], 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca344b57-0592-4727-a484-f31453e996c8",
   "metadata": {},
   "source": [
    "#### To evaluate and compare the results from different pooling strategies, we can take several approaches to analyze how well each strategy preserves the meaning or aligns with the original tokenized text. We can measure the similarity between the embeddings produced by each pooling strategy and the original token embeddings or between embeddings from different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "37d92508-735a-4b6b-8896-bee6924aa877",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_cls_cls = cosine_similarity(cls_pooling[0].cpu().detach().numpy(), cls_pooling[0].cpu().detach().numpy())\n",
    "\n",
    "# this has to be one just as a validation\n",
    "assert similarity_cls_cls == 1, \"Wrong implementation for cosine similarity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3cad9755-e538-42f4-88b3-408a1304052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_cls_eos = cosine_similarity(cls_pooling[0].cpu().detach().numpy(), eos_pooling[0].cpu().detach().numpy())\n",
    "similarity_cls_max = cosine_similarity(cls_pooling[0].cpu().detach().numpy(), max_pooling[0].cpu().detach().numpy())\n",
    "similarity_cls_mean = cosine_similarity(cls_pooling[0].cpu().detach().numpy(), mean_pooling[0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4e36541d-2a04-4546-bce3-a93372f73a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between CLS and EOS pooled embeddings: 0.2897\n",
      "Cosine similarity between CLS and MAX pooled embeddings: 0.1315\n",
      "Cosine similarity between CLS and MEAN pooled embeddings: 0.8014\n"
     ]
    }
   ],
   "source": [
    "print(f'Cosine similarity between CLS and EOS pooled embeddings: {similarity_cls_eos:.4f}')\n",
    "print(f'Cosine similarity between CLS and MAX pooled embeddings: {similarity_cls_max:.4f}')\n",
    "print(f'Cosine similarity between CLS and MEAN pooled embeddings: {similarity_cls_mean:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c3be80-4d14-4022-877a-499eca2472ae",
   "metadata": {},
   "source": [
    "The mean pooled is the closest in similarity to the CLS token."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
